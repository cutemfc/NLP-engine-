{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6027243",
   "metadata": {},
   "source": [
    "# Applying Bert-base-uncased model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97b722",
   "metadata": {},
   "source": [
    "# Goal : Test the Bert-based-uncased model\n",
    "### Step 1: Intent prediction: house, apartment, rooms\n",
    "### Step 2: Slot filling:  only slot filling and modifed with extract slot\n",
    "### Step 3: Combine with the user intent prediction and slot filling using extract slot\n",
    "### Conclusion:\n",
    "#### The prediction performance is currently suboptimal. This is primarily due to the limited size of the training dataset and the high similarity between intent labels, which makes it difficult for the model to distinguish between them accurately. If we plan to deploy this model, further refinement is necessary. In particular, we should consider applying a fine-tuning approach with a more diverse and balanced dataset to improve both intent classification and slot extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ca3774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (4.57.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (4.1.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9333357",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a734a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Step 1. Import necessary packages\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0f0335d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 47.57 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: shared room\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# âœ… Step 2. Define  texts and labels\n",
    "texts = [\n",
    "    \"Looking for 2 bedroom apartment in Berlin, price less than 1000 Euro\",\n",
    "    \"Need a house near school and bus stops, price less than 500 Euro\",\n",
    "    \"Looking for shared room in city center\"\n",
    "    \" Need a house with garten, price less than 2000 Euro\"\n",
    "]\n",
    "intent_labels = [\"apartment\", \"house\", \"shared room\"]\n",
    "\n",
    "# Convert labels to numeric IDs\n",
    "intent_label2id = {label: i for i, label in enumerate(intent_labels)}\n",
    "intent_id2label = {i: label for label, i in intent_label2id.items()}\n",
    "\n",
    "# Sample label IDs for demonstration\n",
    "labels = [0, 1, 2]\n",
    "\n",
    "# âœ… Step 3. Load tokenizer and model\n",
    "intent_model_name = \"bert-base-uncased\"\n",
    "intent_tokenizer = AutoTokenizer.from_pretrained(intent_model_name)\n",
    "\n",
    "model_intent = AutoModelForSequenceClassification.from_pretrained(\n",
    "    intent_model_name,\n",
    "    num_labels=len(intent_labels),\n",
    "    id2label=intent_id2label,\n",
    "    label2id=intent_label2id\n",
    ")\n",
    "\n",
    "# âœ… Step 4. Tokenize the dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"text\": texts,\n",
    "    \"label\": labels\n",
    "})\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return intent_tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# âœ… Step 5. Training setup (CPU friendly)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./intent_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    use_cpu= True  # âœ… force CPU mode\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_intent,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "# âœ… Step 6. Fine-tune (small example)\n",
    "trainer.train()\n",
    "\n",
    "# âœ… Step 7. Test prediction\n",
    "test_text = \"Looking for a room near Ubahn\"\n",
    "inputs = intent_tokenizer(test_text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model_intent(**inputs)\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"Predicted intent: {intent_id2label[predicted_label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "96c7814e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted intent: apartment\n"
     ]
    }
   ],
   "source": [
    "# test the text\n",
    "test_text=\" I need a shared room with kitchen and nearby transport station\"\n",
    "inputs = intent_tokenizer(test_text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model_intent(**inputs)\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"Predicted intent: {intent_id2label[predicted_label]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64c12b",
   "metadata": {},
   "source": [
    "# the prediction is not well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a04b3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define slot labels\n",
    "slot_labels = [\"O\", \"B-location\", \"I-location\", \"B-price\", \"I-price\", \"B-feature\", \"I-feature\"]\n",
    "id2label = {i: label for i, label in enumerate(slot_labels)}\n",
    "label2id = {label: i for i, label in enumerate(slot_labels)}\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_slot = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(slot_labels))\n",
    "\n",
    "# Example text for tokenization\n",
    "texts = [\n",
    "    \"Looking for a 2 bedroom apartment in Berlin, price less than 1000 Euro\",\n",
    "    \"Need a house near school and bus stops, price less than 500 Euro\",\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    [\"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"O\", \"B-location\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"I-price\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-education\", \"O\", \"B-transport\", \"I-transport\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\"]\n",
    "]\n",
    "\n",
    "# Convert dataset\n",
    "dataset = Dataset.from_dict({\"tokens\": texts, \"labels\": labels})\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=False,\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = []\n",
    "    for label in examples[\"labels\"]:\n",
    "        tokenized_inputs[\"labels\"].append([label2id.get(l, 0) for l in label])\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./slot_filling_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    do_train=True,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_slot,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6140bd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./slot_filling_model/tokenizer_config.json',\n",
       " './slot_filling_model/special_tokens_map.json',\n",
       " './slot_filling_model/vocab.txt',\n",
       " './slot_filling_model/added_tokens.json',\n",
       " './slot_filling_model/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model_slot.save_pretrained(\"./slot_filling_model\")\n",
    "tokenizer.save_pretrained(\"./slot_filling_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf877269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./slot_filling_model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# test with the following text\n",
    "#load the model and tokenizer\n",
    "model= AutoModelForSequenceClassification.from_pretrained(\"./slot_filling_model\")\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"./slot_filling_model\")\n",
    "def predict_slots_from_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    word_ids = inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    final_tokens = []\n",
    "    final_labels = []\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue\n",
    "        token = tokens[idx]\n",
    "        label_id = predictions[0][idx].item()\n",
    "        label = id2label[label_id]\n",
    "        final_tokens.append(token)\n",
    "        final_labels.append(label)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    for token, label in zip(final_tokens, final_labels):\n",
    "        print(f\"{token:15} â†’ {label}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "00ae6b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 229.12 examples/s]\n",
      "/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'would', 'like', 'to', 'rent', 'a', 'flat', ',', '100m2', ',', 'price', 'less', 'than', '1000', 'Euro', ',', 'nearby', 'bus', 'stations', 'and', 'parks']\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100, -100, 0, 3, 4, 4, 4, 4, 0, 5, 6, 0, 7, 8, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./slot_filling_model/tokenizer_config.json',\n",
       " './slot_filling_model/special_tokens_map.json',\n",
       " './slot_filling_model/vocab.txt',\n",
       " './slot_filling_model/added_tokens.json',\n",
       " './slot_filling_model/tokenizer.json')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define expanded slot labels\n",
    "slot_labels = [\n",
    "    \"O\",\n",
    "    \"B-size\", \"I-size\",\n",
    "    \"B-price\", \"I-price\",\n",
    "    \"B-transport\", \"I-transport\",\n",
    "    \"B-parks\", \"I-parks\",\n",
    "    \"B-location\", \"I-location\",\n",
    "    \"B-feature\", \"I-feature\"\n",
    "]\n",
    "label2id = {label: i for i, label in enumerate(slot_labels)}\n",
    "id2label = {i: label for i, label in enumerate(slot_labels)}\n",
    "\n",
    "# Sample training data\n",
    "texts = [\n",
    "    [\"I\", \"would\", \"like\", \"to\", \"rent\", \"a\", \"flat\", \",\", \"100m2\", \",\", \"price\", \"less\", \"than\", \"1000\", \"Euro\", \",\", \"nearby\", \"bus\", \"stations\", \"and\", \"parks\"],\n",
    "    [\"Looking\", \"for\", \"a\", \"house\", \"near\", \"public\", \"transport\", \"and\", \"green\", \"areas\", \",\", \"price\", \"below\", \"800\", \"Euro\", \",\", \"size\", \"around\", \"90m2\"],\n",
    "    [\"Need\", \"an\", \"apartment\", \"with\", \"size\", \"of\", \"120\", \"square\", \"meters\", \"and\", \"price\", \"under\", \"950\", \"Euro\", \",\", \"close\", \"to\", \"parks\"],\n",
    "    [\"Searching\", \"for\", \"a\", \"studio\", \"near\", \"metro\", \"and\", \"bus\", \"lines\", \",\", \"price\", \"not\", \"more\", \"than\", \"1100\", \"Euro\", \",\", \"size\", \"about\", \"85m2\"],\n",
    "    [\"Want\", \"a\", \"place\", \"with\", \"100m2\", \"space\", \",\", \"price\", \"around\", \"1000\", \"Euro\", \",\", \"must\", \"be\", \"near\", \"parks\", \"and\", \"transport\"],\n",
    "    [\"Looking\", \"to\", \"rent\", \"a\", \"90m2\", \"flat\", \"close\", \"to\", \"green\", \"areas\", \"and\", \"public\", \"transportation\", \",\", \"budget\", \"is\", \"below\", \"950\", \"Euro\"],\n",
    "    [\"Interested\", \"in\", \"a\", \"house\", \"with\", \"size\", \"near\", \"110m2\", \",\", \"price\", \"limit\", \"is\", \"1000\", \"Euro\", \",\", \"prefer\", \"location\", \"near\", \"bus\", \"routes\"],\n",
    "    [\"Seeking\", \"a\", \"rental\", \"property\", \"of\", \"95m2\", \",\", \"price\", \"maximum\", \"900\", \"Euro\", \",\", \"should\", \"be\", \"close\", \"to\", \"parks\", \"and\", \"public\", \"transport\"],\n",
    "    [\"Need\", \"a\", \"flat\", \"with\", \"size\", \"around\", \"105m2\", \",\", \"price\", \"not\", \"exceeding\", \"980\", \"Euro\", \",\", \"near\", \"green\", \"spaces\", \"and\", \"bus\", \"access\"],\n",
    "    [\"Looking\", \"for\", \"a\", \"90m2\", \"apartment\", \",\", \"price\", \"under\", \"1000\", \"Euro\", \",\", \"must\", \"be\", \"near\", \"parks\", \"and\", \"transport\", \"options\"]\n",
    "]\n",
    "labels = [\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-transport\", \"I-transport\", \"O\", \"B-parks\", \"I-parks\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-transport\", \"I-transport\", \"I-transport\", \"O\", \"B-parks\", \"I-parks\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-size\", \"I-size\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"I-size\", \"I-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-parks\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-transport\", \"I-transport\", \"O\", \"I-transport\", \"I-transport\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-size\", \"I-size\", \"I-size\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-parks\", \"O\", \"B-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"O\", \"B-parks\", \"I-parks\", \"O\", \"B-transport\", \"I-transport\", \"O\", \"B-price\", \"O\", \"I-price\", \"I-price\", \"I-price\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"O\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-transport\", \"I-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"O\", \"B-parks\", \"O\", \"B-transport\", \"I-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-parks\", \"I-parks\", \"O\", \"B-transport\", \"I-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-parks\", \"O\", \"B-transport\", \"I-transport\", \"O\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_dict({\"tokens\": texts, \"labels\": labels})\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "model_slot = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(slot_labels), id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Tokenization and label alignment\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized_inputs.encodings[0].word_ids\n",
    "    label_ids = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            if word_idx < len(example[\"labels\"]):\n",
    "               label_ids.append(label2id[example[\"labels\"][word_idx]])\n",
    "            else:label_ids.append(-100)\n",
    "\n",
    "        else:\n",
    "            label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./slot_filling_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    do_train=True,\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model_slot,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    "\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(tokenized_dataset[0][\"tokens\"])\n",
    "print(tokenized_dataset[0][\"labels\"])\n",
    "\n",
    "# Save model and tokenizer\n",
    "model_slot.save_pretrained(\"./slot_filling_model\")\n",
    "tokenizer.save_pretrained(\"./slot_filling_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1155ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f499f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': None, 'price': None, 'transport': 0.9, 'parks': 0.9}\n"
     ]
    }
   ],
   "source": [
    "text = \"I would like to rent a flat, 100m2, price less than 1000 Euro, and nearby bus stations and parks\"\n",
    "result = extract_slots(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21db600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5991bd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'size': None, 'price': None, 'transport': 0.0, 'parks': 0.0}, {'size': None, 'price': None, 'transport': 0.0, 'parks': 0.0}, {'size': None, 'price': None, 'transport': 0.9, 'parks': 0.0}, {'size': None, 'price': None, 'transport': 0.0, 'parks': 0.0}, {'size': None, 'price': None, 'transport': 0.0, 'parks': 0.0}, {'size': None, 'price': None, 'transport': 0.9, 'parks': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"requests.csv\")\n",
    "df.head()\n",
    "text=df[\"request\"].tolist()\n",
    "result= [extract_slots(t) for t in text]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8567d",
   "metadata": {},
   "source": [
    "# Tune the model and add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e27b6abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define expanded slot labels\n",
    "slot_labels = [\n",
    "    \"O\",\n",
    "    \"B-size\", \"I-size\",\n",
    "    \"B-price\", \"I-price\",\n",
    "    \"B-transport\", \"I-transport\",\n",
    "    \"B-parks\", \"I-parks\",\n",
    "    \"B-location\", \"I-location\",\n",
    "    \"B-feature\", \"I-feature\"\n",
    "]\n",
    "label2id = {label: i for i, label in enumerate(slot_labels)}\n",
    "id2label = {i: label for i, label in enumerate(slot_labels)}\n",
    "\n",
    "# Sample training data\n",
    "texts = [\n",
    "    [\"I\", \"would\", \"like\", \"to\", \"rent\", \"a\", \"flat\", \",\", \"100m2\", \",\", \"price\", \"less\", \"than\", \"1000\", \"Euro\", \",\", \"nearby\", \"bus\", \"stations\", \"and\", \"parks\"],\n",
    "    [\"Looking\", \"for\", \"a\", \"house\", \"near\", \"public\", \"transport\", \"and\", \"green\", \"areas\", \",\", \"price\", \"below\", \"800\", \"Euro\", \",\", \"size\", \"around\", \"90m2\"],\n",
    "    [\"Need\", \"an\", \"apartment\", \"with\", \"size\", \"of\", \"120\", \"square\", \"meters\", \"and\", \"price\", \"under\", \"950\", \"Euro\", \",\", \"close\", \"to\", \"parks\"],\n",
    "    [\"Searching\", \"for\", \"a\", \"studio\", \"near\", \"metro\", \"and\", \"bus\", \"lines\", \",\", \"price\", \"not\", \"more\", \"than\", \"1100\", \"Euro\", \",\", \"size\", \"about\", \"85m2\"],\n",
    "    [\"Want\", \"a\", \"place\", \"with\", \"100m2\", \"space\", \",\", \"price\", \"around\", \"1000\", \"Euro\", \",\", \"must\", \"be\", \"near\", \"parks\", \"and\", \"transport\"],\n",
    "    [\"Looking\", \"to\", \"rent\", \"a\", \"90m2\", \"flat\", \"close\", \"to\", \"green\", \"areas\", \"and\", \"public\", \"transportation\", \",\", \"budget\", \"is\", \"below\", \"950\", \"Euro\"],\n",
    "    [\"Interested\", \"in\", \"a\", \"house\", \"with\", \"size\", \"near\", \"110m2\", \",\", \"price\", \"limit\", \"is\", \"1000\", \"Euro\", \",\", \"prefer\", \"location\", \"near\", \"bus\", \"routes\"],\n",
    "    [\"Seeking\", \"a\", \"rental\", \"property\", \"of\", \"95m2\", \",\", \"price\", \"maximum\", \"900\", \"Euro\", \",\", \"should\", \"be\", \"close\", \"to\", \"parks\", \"and\", \"public\", \"transport\"],\n",
    "    [\"Need\", \"a\", \"flat\", \"with\", \"size\", \"around\", \"105m2\", \",\", \"price\", \"not\", \"exceeding\", \"980\", \"Euro\", \",\", \"near\", \"green\", \"spaces\", \"and\", \"bus\", \"access\"],\n",
    "    [\"Looking\", \"for\", \"a\", \"90m2\", \"apartment\", \",\", \"price\", \"under\", \"1000\", \"Euro\", \",\", \"must\", \"be\", \"near\", \"parks\", \"and\", \"transport\", \"options\"]\n",
    "]\n",
    "labels = [\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-transport\", \"I-transport\", \"O\", \"B-parks\", \"I-parks\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-transport\", \"I-transport\", \"I-transport\", \"O\", \"B-parks\", \"I-parks\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-size\", \"I-size\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"I-size\", \"I-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-parks\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-transport\", \"I-transport\", \"O\", \"I-transport\", \"I-transport\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-size\", \"I-size\", \"I-size\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-parks\", \"O\", \"B-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"O\", \"B-parks\", \"I-parks\", \"O\", \"B-transport\", \"I-transport\", \"O\", \"B-price\", \"O\", \"I-price\", \"I-price\", \"I-price\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"O\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-transport\", \"I-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"O\", \"B-parks\", \"O\", \"B-transport\", \"I-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"B-parks\", \"I-parks\", \"O\", \"B-transport\", \"I-transport\"],\n",
    "    [\"O\", \"O\", \"O\", \"B-size\", \"I-size\", \"O\", \"B-price\", \"I-price\", \"I-price\", \"I-price\", \"O\", \"O\", \"O\", \"B-parks\", \"O\", \"B-transport\", \"I-transport\", \"O\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_dict({\"tokens\": texts, \"labels\": labels})\n",
    "\n",
    "# Load tokenizer and model\n",
    "slot_model_name = \"bert-base-uncased\"\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(slot_model_name, use_fast=True)\n",
    "data_collator = DataCollatorForTokenClassification(slot_tokenizer)\n",
    "model_slot = AutoModelForTokenClassification.from_pretrained(slot_model_name, num_labels=len(slot_labels), id2label=id2label, label2id=label2id)\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = slot_tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(label2id[example[\"labels\"][word_idx]])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "30c4a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_slot = AutoModelForTokenClassification.from_pretrained(\"./slot_filling_model\")\n",
    "slot_tokenizer = AutoTokenizer.from_pretrained(\"./slot_filling_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb02102",
   "metadata": {},
   "source": [
    "# the model is still missed to get some keywords, therefore we combined with the rule-based NLP keyword matching (\"bus\", \"park\") and regex as backup when the model misses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2df8d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slots(text):\n",
    "    words = text.split()\n",
    "    inputs = slot_tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_slot(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)[0]\n",
    "    word_ids = inputs.encodings[0].word_ids\n",
    "\n",
    "    slots = {\n",
    "        \"size\": None,\n",
    "        \"price\": None,\n",
    "        \"transport\": 0.0,\n",
    "        \"parks\": 0.0,\n",
    "        \"university\":0.0,\n",
    "        \"kindergarten\":0.0\n",
    "    }\n",
    "\n",
    "    current_entity = None\n",
    "    current_value = \"\"\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        label_id = predictions[idx].item()\n",
    "        label = id2label[label_id]\n",
    "        word = words[word_idx]\n",
    "\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity and current_value:\n",
    "                if current_entity in [\"size\", \"price\"]:\n",
    "                    digits = re.findall(r\"\\d+\", current_value)\n",
    "                    if digits:\n",
    "                        slots[current_entity] = int(digits[0])\n",
    "            current_entity = label[2:]\n",
    "            current_value = word\n",
    "            if current_entity in [\"transport\", \"parks\"]:\n",
    "                slots[current_entity] = 0.9\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            current_value += \" \" + word\n",
    "        else:\n",
    "            current_entity = None\n",
    "            current_value = \"\"\n",
    "\n",
    "    # Final entity after loop\n",
    "    if current_entity and current_value:\n",
    "        if current_entity in [\"size\", \"price\"]:\n",
    "            digits = re.findall(r\"\\d+\", current_value)\n",
    "            if digits:\n",
    "                slots[current_entity] = int(digits[0])\n",
    "\n",
    "    # Keyword fallback\n",
    "    if 'bus' in text.lower() or \"transport\" in text.lower():\n",
    "        slots[\"transport\"] = 0.9\n",
    "    if \"park\" in text.lower():\n",
    "        slots[\"parks\"] = 0.9\n",
    "    if slots[\"size\"] is None:\n",
    "        match = re.search(r\"\\b(\\d+)\\s?(m2|square meters)\\b\", text.lower())\n",
    "        if match:\n",
    "            slots[\"size\"] = int(match.group(1))\n",
    "\n",
    "    if slots[\"price\"] is None:\n",
    "        match = re.search(r\"\\b(\\d+)\\s?(euro|â‚¬|dollars|Euro)\\b\", text.lower())\n",
    "        if match:\n",
    "            slots[\"price\"] = int(match.group(1))\n",
    "\n",
    "\n",
    "    return slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d4cf42f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': 100, 'price': 1000, 'transport': 0.9, 'parks': 0.9, 'university': 0.0, 'kindergarten': 0.0}\n"
     ]
    }
   ],
   "source": [
    "text = \"I would like to rent a flat, 100m2, price less than 1000 Euro, and nearby bus stations and parks\"\n",
    "result = extract_slots(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c3281da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size': None, 'price': 2000, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}\n"
     ]
    }
   ],
   "source": [
    "text='I would like a apartment nearby the Mitte of Berlin, rent price less than 2000 dollars'\n",
    "result = extract_slots(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "095fd78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I would like a apartment nearby the Mitte of Berlin, rent price less than 2000', 'I would like a small room but can have a pet , price is less than $200', 'I like a 2 rooms and neaby MRT, bus stations. The price is less than $2000', 'I would like to 3 bedrooms, nearby the Mitte District, and rent price less than 2000', 'I would like to have 100m2 and price less than 1000 Euro', 'I would like an apartment 100m2 , price less than 1000 Euro, nearby kindergarten and transport stops'] [{'size': None, 'price': None, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}, {'size': None, 'price': None, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}, {'size': None, 'price': None, 'transport': 0.9, 'parks': 0.9, 'university': 0.0, 'kindergarten': 0.0}, {'size': None, 'price': None, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}, {'size': 100, 'price': 1000, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}, {'size': 100, 'price': 1000, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"requests.csv\")\n",
    "df.head()\n",
    "text=df[\"request\"].tolist()\n",
    "result= [extract_slots(t) for t in text]\n",
    "print(text, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0290b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de489eb9",
   "metadata": {},
   "source": [
    "# Integrate with the inetent prediction and slot filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a99468ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine intent and slot filliing using existing extract_slots()\n",
    "def predict_intent_and_slots(text):\n",
    "    # --- Intent Prediction ---\n",
    "    inputs_intent = intent_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs_intent = model_intent(**inputs_intent)\n",
    "        logits = outputs_intent.logits\n",
    "        if logits.dim() == 3:  # safety fallback\n",
    "            logits = logits.mean(dim=1)\n",
    "        predicted_intent_id = torch.argmax(logits, dim=1).item()\n",
    "        predicted_intent = model_intent.config.id2label[predicted_intent_id]\n",
    "    \n",
    "    # --- Slot Extraction (reuse your existing function) ---\n",
    "    slots = extract_slots(text)\n",
    "    \n",
    "    return predicted_intent, slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1ab8c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Intent: shared room\n",
      "Extracted Slots: {'size': None, 'price': 1000, 'transport': 0.9, 'parks': 0.9, 'university': 0.0, 'kindergarten': 0.0}\n"
     ]
    }
   ],
   "source": [
    "text = \"Looking for a 2 bedroom apartment near bus stops and parks, price less than 1000 Euro\"\n",
    "intent, slots = predict_intent_and_slots(text)\n",
    "\n",
    "print(\"Predicted Intent:\", intent)\n",
    "print(\"Extracted Slots:\", slots)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2813002f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Intent: shared room\n",
      "Extracted Slots: {'size': 100, 'price': 2000, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}\n"
     ]
    }
   ],
   "source": [
    "text=\"I would like to 3 bedrooms, about 100m2 nearby the Mitte District, and rent price less than 2000euro\"\n",
    "intent, slots = predict_intent_and_slots(text)\n",
    "\n",
    "print(\"Predicted Intent:\", intent)\n",
    "print(\"Extracted Slots:\", slots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3e071841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine  the intent and slot filling\n",
    "def predict_intent_and_slots(text):\n",
    "    # --- Intent Prediction ---\n",
    "    inputs_intent = intent_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs_intent = model_intent(**inputs_intent)\n",
    "        logits = outputs_intent.logits\n",
    "        if logits.dim() == 3:  # safety fallback\n",
    "            logits = logits.mean(dim=1)\n",
    "        predicted_intent_id = torch.argmax(logits, dim=1).item()\n",
    "        predicted_intent = model_intent.config.id2label[predicted_intent_id]\n",
    "    \n",
    "    # --- Slot Filling ---\n",
    "    tokenized_inputs = slot_tokenizer(\n",
    "        text.split(),\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs_slot = model_slot(**tokenized_inputs)\n",
    "        predictions = torch.argmax(outputs_slot.logits, dim=2).squeeze().tolist()\n",
    "    \n",
    "    # Align tokens with labels\n",
    "    word_ids = tokenized_inputs.encodings[0].word_ids\n",
    "    slot_predictions = []\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        label_id = predictions[idx]\n",
    "        slot_predictions.append((text.split()[word_id], model_slot.config.id2label[label_id]))\n",
    "    \n",
    "    return predicted_intent, slot_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dfe8337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Intent: shared room\n",
      "Slot Predictions: {'size': 100, 'price': 1000, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}\n"
     ]
    }
   ],
   "source": [
    "text = \"I would like an apartment 100m2 , price less than 1000 Euro, nearby kindergarten and transport stops\"\n",
    "\n",
    "intent, slots = predict_intent_and_slots(text)\n",
    "print(\"Predicted Intent:\", intent)\n",
    "print(\"Slot Predictions:\", slots)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "083fb991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Intent: shared room\n",
      "Slot Predictions: {'size': 100, 'price': 2000, 'transport': 0.9, 'parks': 0.0, 'university': 0.0, 'kindergarten': 0.0}\n"
     ]
    }
   ],
   "source": [
    "text = \"I would like a house with garten , size larger than 100m2 , price less than 2000 Euro, nearby University and transport stops\"\n",
    "\n",
    "intent, slots = predict_intent_and_slots(text)\n",
    "print(\"Predicted Intent:\", intent)\n",
    "print(\"Slot Predictions:\", slots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6833d0",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "the prediction is not very well, training the model is too limited. \n",
    "Intent label are similar.\n",
    "If we will run this model, we still need to do some effort to modify it, for example: fine tuning approach\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
