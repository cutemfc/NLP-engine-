{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400a664f",
   "metadata": {},
   "source": [
    "# Goal: applying the Valhalla/distilbart-mnli-12-3 model\n",
    "# Introduction:  \n",
    "### Valhalla is a distilled version of facebook/bart-barge-mnli\n",
    "It uses 12 encoder layers and 3 decoder layers, offering a good balance between performance and efficiency\n",
    "#### A. Model Name: valhalla/distilbart-mnli-12-3\n",
    "#### B. Author: Valhalla (a well-known NLP researcher on Hugging Face)\n",
    "#### C. Base architecture: BART — a sequence-to-sequence transformer model developed by Facebook AI\n",
    "#### D. Type: Distilled BART → smaller and faster version of BART\n",
    "#### E. Trained on: MNLI dataset (Multi-Genre Natural Language Inference)\n",
    "#### It classifies whether the hypothesis is:\n",
    " entailment (text supports it)\n",
    " neutral\n",
    " contradiction\n",
    "#### The zero-shot-classification : this model has been pretrained on large amount of general data. We don#t need to collect and label for new task.\n",
    "#### Instead of retraining on your labels, it tests whether your input “entails” each label phrase.\n",
    "Example:{\n",
    " 'sequence': 'I think this movie is too long and a bit boring.',\n",
    " 'labels': ['negative', 'neutral', 'positive'],\n",
    " 'scores': [0.87, 0.10, 0.03]\n",
    "}\n",
    "# 1. Zero shot classifiction combined with regex:\n",
    "##### It extracts location, size, price, and amenities, then saves everything in a clean JSON format\n",
    "# 2. No Fine Tuning:\n",
    "##### Fine-tuning presents several challenges in this case. The original model is configured to output three classes, which means we would need to reinitialize the base model to accommodate our specific task. Additionally, training with mixed precision (fp16) requires a CUDA-enabled GPU, which may not be available in all environments. For multi-label classification, the model architecture must be modified to use BCEWithLogitsLoss instead of the default loss function. Personally, I believe this model is not well-suited for our application. Therefore, we will proceed with a zero-shot approach instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe6caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (4.57.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/p310/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f521f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/p310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ed3ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ba5904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Introduce the model and build the pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0906b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Housing requests\n",
    "df=pd.read_csv(\"requests.csv\")\n",
    "df.head()\n",
    "df[\"request_lower\"]=df[\"request\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3893bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Request1: i would like a apartment nearby the mitte of berlin, rent price less than 2000\n",
      "\n",
      "Preprocessing Request2: i would like a small room but can have a pet , price is less than $200\n",
      "\n",
      "Preprocessing Request3: i like a 2 rooms and neaby mrt, bus stations. the price is less than $2000\n",
      "\n",
      "Preprocessing Request4: i would like to 3 bedrooms, nearby the mitte district, and rent price less than 2000\n",
      "\n",
      "Preprocessing Request5: i would like to have 100m2 and price less than 1000 euro\n",
      "\n",
      "Preprocessing Request6: i would like an apartment 100m2 , price less than 1000 euro, nearby kindergarten and transport stops\n",
      "District: any\n",
      "Bedroom: any\n",
      "Price: 1000\n",
      "{\n",
      "  \"original_request\": \"i would like an apartment 100m2 , price less than 1000 euro, nearby kindergarten and transport stops\",\n",
      "  \"structured_request\": {\n",
      "    \"district\": \"any\",\n",
      "    \"bedroom\": \"any\",\n",
      "    \"size\": 100,\n",
      "    \"price\": \"1000\",\n",
      "    \"parks\": \"any\",\n",
      "    \"kindergarten\": \">0.3\",\n",
      "    \"public_transport\": \">0.3\",\n",
      "    \"hospitals\": \"any\",\n",
      "    \"banks\": \"any\",\n",
      "    \"restaurants\": \"any\",\n",
      "    \"cafes\": \"any\",\n",
      "    \"bars\": \"any\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "paired_requests=[] \n",
    "# Etract the input text \n",
    "for i, text_lower in enumerate(df[\"request_lower\"], start=1): print(f\"\\nPreprocessing Request{i}: {text_lower}\") \n",
    "# distrct if the district is mentioned in the text, Mitte, Friedrichshain-Kreuzberg, Neukölln, Charlottenburg-Wilmersdorf, Tempelhof-Schöneberg, Treptow-Köpenick, Lichtenberg, Marzahn-Hellersdorf, Spandau, Steglitze-Zehlendorf, Reinickendorf, Pankow, show thg district, if none, show any districts=[\"Mitte\", \"Friedrichshain-Kreuzberg\", \"Neukölln\", \"Charlottenburg-Wilmersdorf\", \"Tempelhof-Schöneberg\", \"Treptow-Köpenick\", \"Lichtenberg\", \"Marzahn-Hellersdorf\", \"Spandau\", \"Steglitze-Zehlendorf\", \"Reinickendorf\", \"Pankow\"] if any(district.lower() in text_lower for district in districts): \n",
    "district=[district for district in districts if district.lower() in text_lower][0] else: district=\"any\" print(\"District:\", district) \n",
    "# size of the apartment, if mentioned, otherwise any \n",
    "bedroom_pattern = r'(\\d+)\\s*(?:bedroom|bedrooms|rooms|room)' \n",
    "bedroom_match = re.search(bedroom_pattern, text_lower) \n",
    "if bedroom_match: bedroom = bedroom_match.group(1) else: bedroom = \"any\" \n",
    "print(\"Bedroom:\", bedroom) \n",
    "# size, default size if not mentioned \n",
    "size_pattern = r'(\\d{2,4})\\s*(?:m2|sqm|square meters|sq meters|sq\\.?m|square metre|size)' \n",
    "size_match = re.search(size_pattern, text_lower) if size_match: size = int(size_match.group(1)) else: size =\"any\" \n",
    "## price, if mentioned, otherwise any\n",
    "price_pattern = r'(\\d{3,5})\\s*(?:eur|euro|€)' price_match = re.search(price_pattern, text_lower) \n",
    "if price_match: price = price_match.group(1) else: price = \"any\" print(\"Price:\", price) \n",
    "# if mentioned about kindergarten, parks, public transport, shopping mall, gym, hospital, school, banks, pharmacy, university, restaurant, cafe, bar,crime rate) \n",
    "labels = [\"kindergarten\", \"parks\", \"public transport\", \"hospitals\", \"schools\", \"banks\", \"universities\", \"restaurants\", \"cafes\", \"bars\"] \n",
    "results_zs=classifier(text_lower, labels) \n",
    "# score >0.9 identify as exist \n",
    "def parse_score(label, results, threshold=0.3): \n",
    "    idx= results[\"labels\"].index(label) \n",
    "    score=results[\"scores\"][idx] \n",
    "    return f\">{threshold}\" if score > threshold else \"any\" \n",
    "\n",
    "kindergarten= parse_score(\"kindergarten\", results_zs) \n",
    "parks= parse_score(\"parks\", results_zs) \n",
    "public_transport=parse_score(\"public transport\",results_zs) \n",
    "hospitals=parse_score(\"hospitals\", results_zs) \n",
    "banks=parse_score(\"banks\",results_zs) \n",
    "universities=parse_score(\"universities\", results_zs) \n",
    "restaurants=parse_score(\"restaurants\", results_zs) \n",
    "cafes=parse_score(\"cafes\", results_zs) \n",
    "bars=parse_score(\"bars\", results_zs) \n",
    "# Output \n",
    "structured_request={ \"district\": district, \"bedroom\": bedroom, \"price\":price, \"size\":size, \"parks\":parks, \"kindergarten\":kindergarten, \"public_transport\":public_transport, \"hospitals\":hospitals, \"banks\":banks, \"restaurants\":restaurants, \"cafes\":cafes, \"bars\": bars } \n",
    "# Pair original with structured \n",
    "paired_requests.append({ \"original_request\": text_lower, \"structured_request\": structured_request })\n",
    " # Show both \n",
    " print(json.dumps(paired_requests[-1], indent=2, ensure_ascii=False)) \n",
    "# ✅ Save all structured requests to a JSON file \n",
    "with open(\"paired_requests.json\", \"w\", encoding=\"utf-8\") as f: json.dump(paired_requests, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba3ec9",
   "metadata": {},
   "source": [
    "# chage to group_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a1ce881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Request 1: i would like a apartment nearby the mitte of berlin, rent price less than 2000\n",
      "District: Mitte\n",
      "Bedroom: any\n",
      "Price: 2000\n",
      "{\n",
      "  \"original_request\": \"i would like a apartment nearby the mitte of berlin, rent price less than 2000\",\n",
      "  \"structured_request\": {\n",
      "    \"district\": \"Mitte\",\n",
      "    \"bedroom\": \"any\",\n",
      "    \"size\": \"any\",\n",
      "    \"price\": \"2000\",\n",
      "    \"family\": \"any\",\n",
      "    \"safety\": \"any\",\n",
      "    \"transport\": \"any\",\n",
      "    \"nature\": \"any\",\n",
      "    \"convenience\": \"any\",\n",
      "    \"healthcare\": \"any\",\n",
      "    \"education\": \"any\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Preprocessing Request 2: i would like a small room but can have a pet , price is less than $200\n",
      "District: any\n",
      "Bedroom: any\n",
      "Price: $200\n",
      "{\n",
      "  \"original_request\": \"i would like a small room but can have a pet , price is less than $200\",\n",
      "  \"structured_request\": {\n",
      "    \"district\": \"any\",\n",
      "    \"bedroom\": \"any\",\n",
      "    \"size\": \"any\",\n",
      "    \"price\": \"$200\",\n",
      "    \"family\": \"any\",\n",
      "    \"safety\": \"any\",\n",
      "    \"transport\": \"any\",\n",
      "    \"nature\": \"any\",\n",
      "    \"convenience\": \"any\",\n",
      "    \"healthcare\": \"any\",\n",
      "    \"education\": \"any\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Preprocessing Request 3: i like a 2 rooms and neaby mrt, bus stations. the price is less than $2000\n",
      "District: any\n",
      "Bedroom: 2\n",
      "Price: $2000\n",
      "{\n",
      "  \"original_request\": \"i like a 2 rooms and neaby mrt, bus stations. the price is less than $2000\",\n",
      "  \"structured_request\": {\n",
      "    \"district\": \"any\",\n",
      "    \"bedroom\": \"2\",\n",
      "    \"size\": \"any\",\n",
      "    \"price\": \"$2000\",\n",
      "    \"family\": \"any\",\n",
      "    \"safety\": \"any\",\n",
      "    \"transport\": \"any\",\n",
      "    \"nature\": \"any\",\n",
      "    \"convenience\": \"any\",\n",
      "    \"healthcare\": \"any\",\n",
      "    \"education\": \"any\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Preprocessing Request 4: i would like to 3 bedrooms, nearby the mitte district, and rent price less than 2000\n",
      "District: Mitte\n",
      "Bedroom: 3\n",
      "Price: 2000\n",
      "{\n",
      "  \"original_request\": \"i would like to 3 bedrooms, nearby the mitte district, and rent price less than 2000\",\n",
      "  \"structured_request\": {\n",
      "    \"district\": \"Mitte\",\n",
      "    \"bedroom\": \"3\",\n",
      "    \"size\": \"any\",\n",
      "    \"price\": \"2000\",\n",
      "    \"family\": \"any\",\n",
      "    \"safety\": \"any\",\n",
      "    \"transport\": \"any\",\n",
      "    \"nature\": \"any\",\n",
      "    \"convenience\": \"any\",\n",
      "    \"healthcare\": \"any\",\n",
      "    \"education\": \"any\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Preprocessing Request 5: i would like to have 100m2 and price less than 1000 euro\n",
      "District: any\n",
      "Bedroom: any\n",
      "Price: 100\n",
      "{\n",
      "  \"original_request\": \"i would like to have 100m2 and price less than 1000 euro\",\n",
      "  \"structured_request\": {\n",
      "    \"district\": \"any\",\n",
      "    \"bedroom\": \"any\",\n",
      "    \"size\": 100,\n",
      "    \"price\": \"100\",\n",
      "    \"family\": \"any\",\n",
      "    \"safety\": \"any\",\n",
      "    \"transport\": \"any\",\n",
      "    \"nature\": \"any\",\n",
      "    \"convenience\": \"any\",\n",
      "    \"healthcare\": \"any\",\n",
      "    \"education\": \"any\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Preprocessing Request 6: i would like an apartment 100m2 , price less than 1000 euro, nearby kindergarten and transport stops\n",
      "District: any\n",
      "Bedroom: any\n",
      "Price: 100\n",
      "{\n",
      "  \"original_request\": \"i would like an apartment 100m2 , price less than 1000 euro, nearby kindergarten and transport stops\",\n",
      "  \"structured_request\": {\n",
      "    \"district\": \"any\",\n",
      "    \"bedroom\": \"any\",\n",
      "    \"size\": 100,\n",
      "    \"price\": \"100\",\n",
      "    \"family\": \"any\",\n",
      "    \"safety\": \"any\",\n",
      "    \"transport\": \"any\",\n",
      "    \"nature\": \"any\",\n",
      "    \"convenience\": \"any\",\n",
      "    \"healthcare\": \"any\",\n",
      "    \"education\": \"any\"\n",
      "  }\n",
      "}\n",
      "\n",
      "✅ All structured requests saved to paired_requests.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize zero-shot classifier\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-3\")\n",
    "\n",
    "paired_requests = []\n",
    "\n",
    "# --- Define grouped semantic labels ---\n",
    "group_labels = {\n",
    "    \"family\": [\"near kindergarten\", \"good for families\", \"children-friendly neighborhood\",\"nearby kindergarten\"],\n",
    "    \"safety\": [\"safe neighborhood\", \"low crime rate\", \"quiet area\"],\n",
    "    \"transport\": [\"near public transport\", \"nearby subway or bus stop\", \"convenient location\",\"Ubahn\",\"bus station\"],\n",
    "    \"nature\": [\"near parks or pools\", \"green surroundings\", \"recreational areas nearby\"],\n",
    "    \"convenience\": [\"near shops or restaurants or bars\", \"cafes nearby\", \"shopping area nearby\"],\n",
    "    \"healthcare\": [\"near hospital or clinic\", \"good medical facilities nearby\"],\n",
    "    \"education\": [\"near school\", \"near university or college\"],\n",
    "   \n",
    "}\n",
    "\n",
    "# --- Loop through user requests ---\n",
    "for i, text_lower in enumerate(df[\"request_lower\"], start=1):\n",
    "    print(f\"\\nPreprocessing Request {i}: {text_lower}\")\n",
    "\n",
    "    # --- Extract district ---\n",
    "    districts = [\n",
    "        \"Mitte\", \"Friedrichshain-Kreuzberg\", \"Neukölln\", \"Charlottenburg-Wilmersdorf\",\n",
    "        \"Tempelhof-Schöneberg\", \"Treptow-Köpenick\", \"Lichtenberg\", \"Marzahn-Hellersdorf\",\n",
    "        \"Spandau\", \"Steglitze-Zehlendorf\", \"Reinickendorf\", \"Pankow\"\n",
    "    ]\n",
    "    if any(district.lower() in text_lower for district in districts):\n",
    "        district = [district for district in districts if district.lower() in text_lower][0]\n",
    "    else:\n",
    "        district = \"any\"\n",
    "    print(\"District:\", district)\n",
    "\n",
    "    # --- Extract number of bedrooms ---\n",
    "    bedroom_pattern = r'(\\d+)\\s*(?:bedroom|bedrooms|rooms|room)'\n",
    "    bedroom_match = re.search(bedroom_pattern, text_lower)\n",
    "    bedroom = bedroom_match.group(1) if bedroom_match else \"any\"\n",
    "    print(\"Bedroom:\", bedroom)\n",
    "\n",
    "    # --- Extract size (m²) ---\n",
    "    size_pattern = r'(\\d{2,4})\\s*(?:m2|sqm|square meters|sq meters|sq\\.?m|square metre|size)'\n",
    "    size_match = re.search(size_pattern, text_lower)\n",
    "    size = int(size_match.group(1)) if size_match else \"any\"\n",
    "\n",
    "    # --- Extract price (€) ---\n",
    "    price_pattern = r'(?:less than|under|below|<|up to |maximum|max)?\\s*([$€]?\\s*\\d{2,5})\\s*(?:eur|euro|€|usd|dollars)?'\n",
    "    price_match = re.search(price_pattern, text_lower)\n",
    "    price = price_match.group(1) if price_match else \"any\"\n",
    "    print(\"Price:\", price)\n",
    "\n",
    "    # --- Run zero-shot classification across all group labels ---\n",
    "    all_labels = [label for group in group_labels.values() for label in group]\n",
    "    results_zs = classifier(text_lower, all_labels)\n",
    "\n",
    "    # --- Function to parse scores for each group ---\n",
    "    def parse_group_score(results, group_label_list, threshold=0.7):\n",
    "        scores = []\n",
    "        for label in group_label_list:\n",
    "            if label in results[\"labels\"]:\n",
    "                idx = results[\"labels\"].index(label)\n",
    "                scores.append(results[\"scores\"][idx])\n",
    "        return \">0.8\" if scores and max(scores) > threshold else \"any\"\n",
    "\n",
    "    # --- Evaluate each group ---\n",
    "    parsed_groups = {\n",
    "        group_name: parse_group_score(results_zs, group_labels[group_name], threshold=0.8)\n",
    "        for group_name in group_labels\n",
    "    }\n",
    "\n",
    "    # --- Structured output ---\n",
    "    structured_request = {\n",
    "        \"district\": district,\n",
    "        \"bedroom\": bedroom,\n",
    "        \"size\": size,\n",
    "        \"price\": price,\n",
    "    }\n",
    "    structured_request.update(parsed_groups)\n",
    "\n",
    "    # --- Pair original with structured ---\n",
    "    paired_requests.append({\n",
    "        \"original_request\": text_lower,\n",
    "        \"structured_request\": structured_request\n",
    "    })\n",
    "\n",
    "    # --- Print result for current request ---\n",
    "    print(json.dumps(paired_requests[-1], indent=2, ensure_ascii=False))\n",
    "\n",
    "# --- Save all structured requests to JSON file ---\n",
    "with open(\"paired_requests.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(paired_requests, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n✅ All structured requests saved to paired_requests.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5091da0",
   "metadata": {},
   "source": [
    "### The Result can check the paired_request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242eeee0",
   "metadata": {},
   "source": [
    "# Uning Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6eebe56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using fine tuning , install packages\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoConfig, AutoModelForAudioClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ccbccb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForSequenceClassification\n",
    "\n",
    "class BartForSeqClsWithExtraKwargs(BartForSequenceClassification):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Remove any unexpected kwargs\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "        return super().forward(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9eea7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSeqClsWithExtraKwargs were not initialized from the model checkpoint at valhalla/distilbart-mnli-12-3 and are newly initialized because the shapes did not match:\n",
      "- classification_head.out_proj.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([7, 1024]) in the model instantiated\n",
      "- classification_head.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 267.59 examples/s]\n",
      "/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 89\u001b[0m\n\u001b[1;32m     82\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     83\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     84\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     85\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# ✅ Step 7: Fine-tune\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# ✅ Step 8: Save LoRA-adapted model\u001b[39;00m\n\u001b[1;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal_estate_lora_model_distibart\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/peft/peft_model.py:1652\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1651\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1652\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1663\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1665\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m, in \u001b[0;36mBartForSeqClsWithExtraKwargs.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Remove any unexpected kwargs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1653\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1652\u001b[0m         loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n\u001b[0;32m-> 1653\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1655\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/loss.py:828\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/functional.py:3597\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3595\u001b[0m     )\n\u001b[0;32m-> 3597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3598\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\n\u001b[1;32m   3599\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Step 2: Load tokenizer and base model\n",
    "model_name = \"valhalla/distilbart-mnli-12-3\"\n",
    "num_labels=7\n",
    "config= AutoConfig.from_pretrained(model_name, num_labels=num_labels, problem_type=\"multi_label_classification\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model=BartForSeqClsWithExtraKwargs.from_pretrained(model_name, config=config, ignore_mismatched_sizes= True)# original size is 3\n",
    "# Number of groups as labels\n",
    "group_labels = {\n",
    "    \"family\": [\"near kindergarten\", \"good for families\", \"children-friendly neighborhood\",\"nearby kindergarten\"],\n",
    "    \"safety\": [\"safe neighborhood\", \"low crime rate\", \"quiet area\"],\n",
    "    \"transport\": [\"near public transport\", \"nearby subway or bus stop\", \"convenient location\",\"Ubahn\",\"bus station\"],\n",
    "    \"nature\": [\"near parks or pools\", \"green surroundings\", \"recreational areas nearby\"],\n",
    "    \"convenience\": [\"near shops or restaurants or bars\", \"cafes nearby\", \"shopping area nearby\"],\n",
    "    \"healthcare\": [\"near hospital or clinic\", \"good medical facilities nearby\"],\n",
    "    \"education\": [\"near school\", \"near university or college\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# ✅ Step 3: Add LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ✅ Step 4: Create small training dataset\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I want an apartment near kindergarten and transport stops, less crime.\",\n",
    "        \"Looking for a house far from city center, very safe, near school and hospitals.\",\n",
    "        \"Apartment under 1000 euro, no kids, doesn’t matter about transport.\",\n",
    "        \"I want an apartment near a kindergarten and subway, safe area, under 1000 Euro\",\n",
    "        \"Two bedrooms close to subway, low crime, nearby parks, price less than 1200 euro.\",\n",
    "        \"2 rooms nearby supermarket and bars or restaurants, price less than 800 Euro.\",\n",
    "        \"3-bedrooms, near public transport, shopping malls, and safe area\",\n",
    "        \"Apartment near a hospital and clinic, good medical facilities, quiet area.\",\n",
    "        \"Looking for a flat close to a hospital and pharmacy, suitable for elderly family members.\"\n",
    "    ],\n",
    "    \"labels\": [\n",
    "        [1,1,1,1,0,0,0],  # family ✅, safety ✅, transport ✅, nature ❌, convenience ❌, healthcare ❌, education ❌\n",
    "        [0,1,0,0,0,1,1],  # safety ✅, education ✅, and healthcare\n",
    "        [0,0,1,0,0,0,0],  # transport ✅\n",
    "        [1,1,1,0,0,0,0],  # family ✅, safety ✅, transport ✅\n",
    "        [0,1,1,1,0,0,0],  # safety ✅, transport ✅, nature ✅\n",
    "        [0,0,1,0,1,0,0],  # transport ✅, convenience ✅\n",
    "        [0,1,1,0,1,0,0],  # safety ✅, transport ✅, convenience ✅\n",
    "        [0,1,0,0,0,1,0],  # safety ✅, healthcare ✅\n",
    "        [0,1,0,0,0,1,0]   # safety ✅, healthcare ✅\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# ✅ Step 5: Tokenize dataset\n",
    "def preprocess(example):\n",
    "    enc = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    enc[\"labels\"] = example[\"labels\"]\n",
    "    return enc\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess)\n",
    "\n",
    "# ✅ Step 6: Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./real_estate_lora_results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "# ✅ Step 7: Fine-tune\n",
    "trainer.train()\n",
    "\n",
    "# ✅ Step 8: Save LoRA-adapted model\n",
    "model.save_pretrained(\"real_estate_lora_model_distibart\")\n",
    "tokenizer.save_pretrained(\"real_estate_lora_model_distibart\")\n",
    "\n",
    "print(\"✅ Fine-tuning completed and model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3edec7",
   "metadata": {},
   "source": [
    "#Fine tuning has some problem, the original model set output three class, we need to reset the the base model (), and requires the fp16 CUDA GPU, for multiple labeling still require to change the model to BCEWithLogistcLoss, I personaly think the model is not good for this case application. We only do zero shot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([3, 1024]) from checkpoint, the shape in current model is torch.Size([4, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalhalla/distilbart-mnli-12-3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ✅ Step 2: Add LoRA\u001b[39;00m\n\u001b[1;32m      7\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      8\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,                    \u001b[38;5;66;03m# rank\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,          \u001b[38;5;66;03m# scaling factor\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEQ_CLS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/modeling_utils.py:5051\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5042\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5044\u001b[0m     (\n\u001b[1;32m   5045\u001b[0m         model,\n\u001b[1;32m   5046\u001b[0m         missing_keys,\n\u001b[1;32m   5047\u001b[0m         unexpected_keys,\n\u001b[1;32m   5048\u001b[0m         mismatched_keys,\n\u001b[1;32m   5049\u001b[0m         offload_index,\n\u001b[1;32m   5050\u001b[0m         error_msgs,\n\u001b[0;32m-> 5051\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5057\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5066\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5067\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5068\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/modeling_utils.py:5471\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5468\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5470\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m-> 5471\u001b[0m         _error_msgs, disk_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5472\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[1;32m   5474\u001b[0m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/modeling_utils.py:847\u001b[0m, in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m--> 847\u001b[0m     disk_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/modeling_utils.py:770\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[1;32m    768\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 770\u001b[0m     \u001b[43m_load_parameter_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;66;03m# TODO naming is stupid it loads it as well\u001b[39;00m\n\u001b[1;32m    774\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/modeling_utils.py:667\u001b[0m, in \u001b[0;36m_load_parameter_into_model\u001b[0;34m(model, param_name, tensor)\u001b[0m\n\u001b[1;32m    665\u001b[0m module, param_type \u001b[38;5;241m=\u001b[39m get_module_from_name(model, param_name)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# This will check potential shape mismatch if skipped before\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mparam_type\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p310/lib/python3.10/site-packages/torch/nn/modules/module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2620\u001b[0m             ),\n\u001b[1;32m   2621\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2627\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m     )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([3, 1024]) from checkpoint, the shape in current model is torch.Size([4, 1024])."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ✅ Step 1: Load tokenizer and base model\n",
    "model_name = \"valhalla/distilbart-mnli-12-3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_labels=6\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "\n",
    "# ✅ Step 2: Add LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # rank\n",
    "    lora_alpha=32,          # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # attention layers to adapt, small matrix\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ✅ Step 3: Create your small training dataset\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I want an apartment near kindergarten and transport stops, less crime.\",\n",
    "        \"Looking for a house far from city center, very safe, near school.\",\n",
    "        \"Apartment under 1000 euro, no kids, doesn’t matter about transport.\",\n",
    "        \"I want an apartment near a kindergarten and subway, safe area, under 1000 Euro, \",\n",
    "        \"Two bedrooms close to subway, low crime, nearby parks, price less than 1200 euro.\",\n",
    "        \"2 rooms nearby supermarket and bars or resturants, price less than 800Euro.\",\n",
    "        \"3-bedrooms, near public transport, shopping malls, and satey area\" \n",
    "      \n",
    "    ],\n",
    "\n",
    "\"labels\" :[\n",
    "    [1, 1, 1, 1, 0, 0],  # Near kindergarten ✅, children-friendly ✅, low crime ✅, transport ✅, parks ❌, shopping ❌\n",
    "    [0, 1, 1, 0, 0, 0],  # Kindergarten ❌, children-friendly ✅, low crime ✅, transport ❌, parks ❌, shopping ❌\n",
    "    [0, 0, 0, 1, 0, 0],  # Kindergarten ❌, children-friendly ❌, low crime ❌, transport ✅, parks ❌, shopping ❌\n",
    "    [1, 1, 1, 1, 0, 0],  # Kindergarten ✅, children-friendly ✅, low crime ✅, transport ✅, parks ❌, shopping ❌\n",
    "    [0, 1, 1, 1, 1, 0],  # Kindergarten ❌, children-friendly ✅, low crime ✅, transport ✅, parks ✅, shopping ❌\n",
    "    [0, 1, 0, 0, 0, 1],  # Kindergarten ❌, children-friendly ✅, low crime ❌, transport ❌, parks ❌, shopping ✅\n",
    "    [0, 1, 1, 1, 0, 1]   # Kindergarten ❌, children-friendly ✅, low crime ✅, transport ✅, parks ❌, shopping ✅\n",
    "]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# ✅ Step 4: Tokenize\n",
    "def preprocess(example):\n",
    "    enc = tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    enc[\"labels\"] = example[\"labels\"]\n",
    "    return enc\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess)\n",
    "\n",
    "# ✅ Step 5: Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distibart-mnli./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "# ✅ Step 6: Fine-tune\n",
    "trainer.train()\n",
    "\n",
    "# ✅ Step 7: Save your LoRA-adapted model\n",
    "model.save_pretrained(\"real_estate_lora_model and distibart\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5eea6dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot classification results: {'sequence': 'text', 'labels': ['price', 'public transport', 'parks', 'district', 'shopping mall', 'bedroom', 'hospital', 'school', 'kindergarten', 'gym'], 'scores': [0.24442839622497559, 0.15627136826515198, 0.11556384712457657, 0.10741044580936432, 0.10244598984718323, 0.07879061996936798, 0.07345880568027496, 0.057474758476018906, 0.03305509313941002, 0.031100604683160782]}\n"
     ]
    }
   ],
   "source": [
    "# Define the labels\n",
    "text=\"I want an apartment near kindergarten and transport stops, less crime.\",\n",
    "labels = [\"district\", \"price\", \"bedroom\", \"kindergarten\", \"parks\", \"public transport\", \"shopping mall\", \"gym\", \"hospital\", \"school\"]\n",
    "results = classifier(\"text\", labels)\n",
    "print(\"Zero shot classification results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
